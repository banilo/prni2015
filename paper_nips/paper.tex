\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{bbm}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{bbm}
\usepackage[titletoc]{appendix}
\usepackage{wrapfig}
\usepackage{afterpage}
\usepackage{amssymb}

\def\B#1{\bm{#1}}
%\def\B#1{\mathbf{#1}}
\def\trans{\mathsf{T}}

%\renewcommand{\labelitemi}{--}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Semi-supervised low-rank logistic regression for
high-dimensional neuroimaging data}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\soft}{soft}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\Prox}{Prox}
\DeclareMathOperator{\im}{im}

% macros from michael's .tex
\DeclareMathOperator{\dist}{dist} % The distance.
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\abs}{abs}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}


\nipsfinalcopy % Uncomment for camera-ready version
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\author{Danilo Bzdok, Michael Eickenberg, Olivier Grisel,
  Bertrand Thirion,
  Ga\"el Varoquaux \\\\\textbf{\textit{email:} }firstname.lastname@inria.fr}

\maketitle

\begin{abstract}
Imaging neuroscience links human behavior to aspects of brain
biology in ever-increasing datasets.
%
Existing neuroimaging methods typically perform either discovery of unknown
neural structure or testing of neural structure associated with mental tasks.
%
However, modelling the neural correlates of mental tasks hinges
on the pertinence of the assumed neurobiological structure.
%
We therefore propose to solve the unsupervised dimensionality reduction
and supervised task classification in
an identical statistical learning problem.
%
We show that this approach yields more accurate and more interpretable
neural models of psychological tasks in a reference dataset.
%

\textbf{keywords}: dimensionality reduction; semi-supervised learning;
bioinformatics; fMRI; systems neuroscience

\end{abstract}

\section{Introduction}
%
Methods used in neuroimaging research can be grouped by discovering
neurobiological structure or assessing the neural correlates associated
with mental tasks.
To discover spatial distributions of activation structure across time,
independent component analysis (ICA) \cite{beckmann2005} is often used
to decompose the BOLD (blood-oxygen level-dependent) signals into the
main modes of variation.
The ensuing spatial activation patterns are believed to represent
brain networks of
functionally interacting brain regions \cite{smith2009}.
Similarly, sparse principal component analysis (SPCA) \cite{varoqu2011}
has been used to
separate BOLD signals into parsimonious network components.
Thus extracted brain networks have been shown to be
manifestations of electrophysiological oscillation frequencies \cite{hipp15}.
Their fundamental organizational role is further
attested by continued covariation during sleep and anesthesia.
%
Network discovery is typically performed by applying ICA or SPCA on
unlabeled "resting-state" data. These capture brain dynamics
during ongoing random thought without controlled environmental stimulation.
A large proportion of the BOLD signal is known
not to correlate with a particular behavior, stimulus, or experimental task
\cite{fox07}. 

On the other hand, to investigate
the neural correlates underlying mental tasks,
the general linear model (GLM) is the dominant approach \cite{friston94}.
The contribution of
individual brain voxels is estimated
according to a design matrix of experimental tasks.
Alternatively, psychophysiological interactions (PPI)
elucidate the functional interactions between voxels as a function
of experimental tasks \cite{friston97}.
Dynamic causal modeling (DCM), in turn, quantifies directed,
task-driven influences between regions
by treating the brain as a nonlinear dynamic system with unknown
neuronal states \cite{stephan04}.
As a last example, always more neuroimaging studies model
experimental tasks by training classification algorithms on brain signals
\cite{poldrack09decoding}.
All these methods are applied to labeled task data that capture brain dynamics
during stimulus-guided behavior.
Two important conclusions can be drawn.
First, the mentioned supervised neuroimaging analyses operate
without exception in a voxel space. This ignores the fact that the BOLD
signal exhibits distributed spatial activation patterns.
Second, existing neuroimaging analyses do not acknowledge the fact that the
task-induced changes of the BOLD signal typically amount to less than 5\%
of baseline activity \cite{fox07}. They do thus not exploit the high similarity
of BOLD dynamics in the human brain at rest and during experimental tasks,
as observed using ICA \cite{smith2009}.

Both these biological properties can be conjointly exploited in a 
semi-supervised (i.e., use rest and task data)
low-rank (i.e., perform network decomposition)
approach.
%
The integration of brain-network discovery into a 
supervised classification framework should identify the most relevant
neurobiological structure for the prediction problem at hand.
%
Autoencoders suggest themselves because they can emulate
variants of most unsupervised learning algorithms,
including PCA, SPCA, and ICA \cite{hinton06}.
Autoencoders
are one-layered learning models that condense the input data to
local and global representations
by improving reconstruction from them.
%
They behave like a PCA
in case of one linear hidden layer and a squared error loss
%
\begin{wrapfigure}{r}{0.40\textwidth}
  \centering
  %\captionsetup{labelformat=nonumber}
    \includegraphics[width=0.40\textwidth]{figures/figure1.png}
  \caption {\textbf{Architecture}
  }
\end{wrapfigure}
%
\cite{baldi1989neural}.
This architecture yields a convex optimization objective
with unique global minimum.
Autoencoders behave like a SPCA if shrinkage terms are added to the
model weights in the optimization objective.
Moreover, they have the characteristics of an ICA in case of tied weights
and feeding the first-layer activation into any nonlinear convex
function \cite{le2011ica}.
These authors further demonstrated that ICA, sparse autoencoders, and 
sparse coding are mathematically equivalent
under mild conditions.
Hence, autoencoders can flexibly project the neuroimaging data
onto the main axes of variation and thus
reverse-engineer properties of the data-generating
neural processes \cite{olshausen96}.

In the present investigation,
a linear autoencoder will be fed by (unlabeled) rest data and
integrated as bottleneck
into a low-rank logistic regression fed by (labeled) task data.
Using the chain rule for gradient descent, we can then
solve the unsupervised data representation and the supervised classification
in an identical statistical learning problem.
From the perspective of dictionary learning, the first layer represents
the discovered set of basis functions
whose linear combinations are learned
by the second layer \cite{olshausen96}.
%
Neurobiologically, this allows 
delineating a low-dimensional manifold of brain network patterns and then 
distinguishing mental tasks
by their most discriminative linear combinations.
%
Theoretically, a reduction in model variance should be achieved by
resting-state autoencoders that
put probability mass on the most neurobiologically
valid models in the hypothesis set.
%
The generalization performance should consequently be improved due to 
reduced Vapnik-Chervonenkis dimensions of the trained estimator.
%
Taken together,
the important modes of variation in brain dynamics and
the neural correlates subserving mental operations
have mostly been studied in isolation.
We provide a principled computational framework to link these previously
unconnected domains of systems neuroscience.


\section{Methods}
%
\paragraph{Data.}
As the currently biggest openly-accessible reference dataset,
we chose the Human Connectome Project (HCP) resources
\cite{barch2013}.
Neuroimaging task data with labels of ongoing cognitive processes
were drawn from 500 unrelated,
healthy HCP participants.
18 HCP tasks 
were selected that are known to elicit reliable neural activity
across participants.
The task paradigms include
1) working memory/cognitive control processing, 2)
incentive processing, 3) visual and somatosensory-motor processing,
4) language processing (semantic and phonological processing),
5) social cognition, 6) relational processing, and 7) emotional
processing. All data were acquired on the same Siemens Skyra 3T scanner.
Whole-brain EPI acquisitions were acquired with a
32 channel head coil (TR=720ms, TE=33.1ms, flip angle=52°, BW=2290Hz/Px,
in-plane FOV=$280\times180$mm, 72 slices, 2.0mm isotropic voxels).
The ``minimally preprocessed'' pipeline includes
gradient unwarping, motion correction, fieldmap-based EPI distortion
correction, brain-boundary-based registration of EPI to structural
T1-weighted scans, nonlinear (FNIRT) registration into MNI space,
and grand-mean intensity normalization. Activation maps were spatially
smoothed by a Gaussian kernel of 4mm (FWHM). A GLM was
implemented by FILM from the FSL suite with model regressors from convolution
with a “canonical” hemodynamic response function and from temporal derivatives.
HCP tasks were conceived to modulate activation
in a maximum of different brain regions and neural systems. Indeed, at
least 70\% of the participants showed consistent brain activity in
contrasts from the task battery, which certifies excellent
coverage \cite{barch2013}.
In sum, the HCP task data incorporated 8650 first-level activity maps
from 18 diverse paradigms administered to 498 participants (2 removed
due to incomplete data).
All maps were resampled to a common 60x72x60 space of
3mm isotropic voxels and gray-matter masked (at least 10\% tissue
probability).
The supservised analyses were based on labeled HCP task maps with
79,941 voxels of interest representing z-values in gray matter.

These labeled data were complemented by unlabeled activity maps
from HCP acquisitions of unconstrained resting-state activity.
These reflect brain activity in the absence of controlled thought.
In line with the goal of the present study, acquisition of these data was
specifically aimed at the study of task-rest correspondence.
From each participant, we included two
time-series for left and right phase encoding
with 1,200 maps of multiband, gradient-echo planar imaging acquired
during a period of 15min (TR=720 ms, TE=33.1 ms, flip angle=52°,
FOV=$280\times180$mm, and 2.0mm isotropic voxels). Besides run duration,
the task acquisitions were identical to the resting-state fMRI acquisitions
for maximal compatibility between task and rest data.
We here drew on ``minimally preprocessed'' rest data
from 200 randomly selected healthy, unrelated participants.
PCA was applied to each set of 1,200 rest maps for
denoising by keeping only the 20 main modes of
variation.
In sum, the HCP rest data concatenated
8000 unlabeled, noise-cleaned rest maps with
40 brain images from each of 200 randomly selected participants.

We were further interested in the utility of the optimal low-rank projection
in one task dataset for dimensionality reduction in another task dataset.
To this end, the HCP-derived network decompositions were used as preliminary
step in the classifcation problem of another large sample.
The ARCHI dataset \cite{pinel07} provides activity maps from
diverse experimental tasks, including auditory and visual perception, motor action,
reading, language comprehension and mental calculation.
81 right-handed healthy participants
(3 not included in present analyses due to incomplete data)
without psychiatric or
neurological history participated in four fMRI sessions acquired under
different experimental paradigms.
The functional maps were warped into
the MNI space and resampled to isotropic 3mm resolution.
Whole-brain EPI data were acquired with the same Siemens Trio with a 32
channel head coil (TR=2400ms, TE=30ms, flip angle=60°, in-plane
FOV=$19.2\times19.2$cm, 40 slices, 3.0mm isotropic voxels).
Standard preprocessing was performed with Nipype \cite{gorgo11}, including
slice timing, motion correction, alignment, and spatial normalization.
Activation maps were spatially smoothed by
a Gaussian kernel of 5mm (FWHM).
Analogous to HCP data, the second task dataset incorporated 1404
labeled, grey-matter masked, and z-scored activity maps
from 18 diverse tasks acquired in 78 participants.

The labeled and unlabeled data were fed into a linear statistical model
composed of autoencoder and low-rank logistic regression.

\paragraph{Linear autoencoder.}
The affine autoencoder takes the input 
$x$ and projects it into coordinates of a latent representation $z$
by

\begin{eqnarray}
  \begin{split}
  z &= W_0 x + b_{W_0} \\
  x' &= W_1 z + b_{W_1}
  \end{split}
  \label{eq:autoenc}
\end{eqnarray}

where $x \in \mathbb{R}^{d}$ denotes the vector of $d=79,941$
voxel values from each
rest map, $z \in \mathbb{R}^{n}$ is the $n$ hidden dimensions (i.e.,
distributed neural activity patterns), and 
$x' \in \mathbb{R}^{d}$ is the reconstruction vector of the original activity map
from the hidden variables. Further, ${W_0}$ denotes the weight matrix that
transforms
from input space into the hidden space (encoder),
${W_1}$ is the weight matrix for back-projection from the hidden variables to the
output space (decoder).
${b_{W_0}}$ and ${b_{W_1}}$ are bias vectors.
Note that ${W_0}$ and ${W_1}$ are tied,
$W_0$ = $W_1^T$.
The optimal model parameters ${W_0}, {b_{W_0}}, {b_{W_1}}$ are found by
minimizing the imperfection of reconstruction according to squared error

\begin{eqnarray}
  {\mathcal{L_{AE}}}(x, x') = || x - x' ||^2
\end{eqnarray}

This reconstruction error criterion equates with
maximizing a lower bound on the mutual information between
input and the learned representation.
Nonlinearities were not applied on the
activations in the first layer.

\paragraph{Reduced-rank logistic regression.}
Lossy compression by a low-dimensional bottleneck
is also imposed by the first layer of the low-rank
multinomial logistic regression.
It gives the probability of an input $x$ to belong
to a class $i \in \mathbb{N}$

\begin{eqnarray}
  \begin{split}
    P(Y=i|x, V_0,V_1,b_{V_0}, b_{V_1}) &= softmax_i(V_1 (V_0 x + b_{V_0}) + b_{V_1}) \\
    &= \frac {e^{V_1_i (V_0_i x + b_{V_0}_i) + b_{V_1}_i}} {\sum_j e^{V_1_j (V_0_j x + b_{V_0}_j) + b_{V_1}_j}}            
  \end{split}
  \label{eq:lr}
\end{eqnarray}

where the matrix $V_0 \in \mathbb{R}^{dxn}$ transforms the input $x \in \mathbb{R}^{d}$
into $n$ latent components
and the matrix $V_1 \in \mathbb{R}^{nxc}$ projects the latent components
onto hyperplanes that reflect $c$ class probabilities.
$b_{V_0}$ and $b_{V_1}$ are corresponding
bias vectors.
The loss function is given by

\begin{eqnarray}
  \begin{split}
    {\mathcal{L_{LR}}}(x, y) = \frac{1}{N_{X_{task}}} \sum_{i=0}^{N_{X_{task}}} \log(P(Y=y^{(i)}|x^{(i)}; V_1 (V_0 x + b_{V_0}) + b_{V_1})
\end{split}
\label{eq:lr_loss}
\end{eqnarray}

\paragraph{Layer combination.}
Importantly, the optimization problem of the linear autoencoder
and the low-rank logistic regression
are linked one two levels. First, their transformation matrices mapping from
input to the latent space are identical
\begin{eqnarray}
  V_0 = W_0
\end{eqnarray}
We thus search for a compression of the 79,941 voxel values into $n$ latent
components that represent an optimal latent code for both
rest and task activity data.
Second, the objectives of the autoencoder and the low-rank
logistic regression are interpolated in the common loss function

\begin{eqnarray}
  \begin{split}
{arg\,min}_\theta \hspace{0.2cm} {\mathcal{L}}(\theta, \lambda) = (\lambda) {\mathcal{L_{LR}}}
+ (1-\lambda)\frac{1}{N_{X_{rest}}} {\mathcal{L_{AE}}} + \ell_1(\theta) + \ell_2(\theta)
  \label{eq:loss_equ}
\end{split}
\end{eqnarray}

In so doing, we search for the combined model parameters
$\theta=\{V_0,V_1,b_{V_0}, b_{V_1}, b_{W_0}, b_{W_1}\}$
with respect to the (unsupervised) reconstruction error and the
(supervised) task classification.
${\mathcal{L_{AE}}}$ is devided by ${N_{X_{rest}}}$ to equilibrate both
loss terms to the same order of magnitude.
For parameter shrinkage, we
impose a combination of $\ell_1$ and $\ell_2$ penalties
$\forall p \in \theta$ (i.e., elasticnet).

\paragraph{Optimization.}
The common objective was approximated in the neuroimaging data
by updating parameters by derivates of the 
semi-supervised low-rank logistic regression.
The required gradients are easily obtained by using the chain rule to
backpropagate error derivatives through the linear network.
As solver, we chose \textit{rmsprop} \cite{rmsprop},
a mini-batch version of rprop.
This procedure dictates an adaptive learning rate
for each model parameter by
scaled gradients from a running average.
Gradient normalization by rmsprop
is known to effectively exploit curvature information.
We opted for a small batch size of $100$, given the high degree of
redundancy in $X_{rest}$ and $X_{task}$.
The matrix parameters were initalized by Gaussian random values multiplied
by a gain of $0.004$. The bias parameters were initalized to zero.
With a slight abuse of notation, let $\theta$ denote a component of $\theta$.
The normalization factor and the update rule for $\theta$
are given by

\begin{eqnarray}
  \begin{split}
v^{(t+1)} &= \rho v^{(t+1)} + (1 - \rho)\left(\frac{\partial f}{\partial \theta}\right)^2
%v^{(t+1)} = \rho v^{(t+1)} + (1 - \rho)˜|\nabla f
\\
\theta^{(t+1)} &= \theta^{(t)} + \alpha \frac{\nabla f(\theta^{(t)})}{\sqrt{v^{(t+1)} + \epsilon}},
  \end{split}
\end{eqnarray}

where $0 < \rho < 1$ constitutes the decay rate. $\rho$ was set to
$0.9$ to deemphasize the magnitude of the gradient.
Further, $\alpha$ is the learning rate and $\epsilon$ a global damping factor.
The hyper-parameter $\alpha$ was set to $0.00001$ by prior manual
cross-validation and $\epsilon$ was set to $10^{-6}$.
%
Note that we have also experimented with other solvers
(stochastic gradient descent, adadelta, and adagrad) but found that
rmsprop converged faster and with higher generalization performance.

\paragraph{Hints.}
In fact, the constraint by a rest-data autoencoder qualifies as a
\textit{hint}
rather than regularization in a strict sense \cite{abu1994hints}.
Its purpose is not to prevent overfitting but to introduce
prior knowledge on known properties of the unknown target function $f$.
Rather than only relying on input-output pairs in the learning process,
we thus narrow our hypothesis set to the biologically most plausible solutions.
That is, we reduce the search space in a way that
is compatible with the expected representation of BOLD activity signals.

\paragraph{Implementation.}
The analyses were performed in Python.
We used \textit{nilearn} to handle
the high-dimensional neuroimaging data 
\cite{abrah14}
and
\textit{theano} for automatic, numerically stable
differentiation of symbolic computation graphs
\cite{bastien2012theano, bergstra2010theano}.
All Python scripts that generated the results are
accessible online for reproducibility and reuse
\url{http://github.com/anonymous/anonymous}.
% url{http://github.com/banilo/nips2015}.

\bigskip

\begin{figure}
\begin{centering}
\includegraphics[width=1.00\textwidth]{figures/accuracies.png}
\end{centering}
\caption{\textbf{Generalization performance}
Depicts in-sample (apply current model on training set, \textit{left column}),
out-of-sample (current model on test set, \textit{middle column}),
and out-of-dataset (compress other dataset using the
learned decomposition for classification, \textit{right column}) accuracy.
Curves for different choices of $\lambda$ as well as for ordinary
logistic regression (\textit{yellowish})
across iterations over the HCP task data (epochs).
}
\end{figure}

\begin{figure}
\begin{centering}
\includegraphics[width=1.00\textwidth]{figures/figure_weights.jpg}
\end{centering}
\caption{\textbf{Comparing the interpretability of classification weights}
The voxel predictors corresponding to 5 (of 18) psychological tasks (\textit{rows})
from the HCP dataset \cite{barch2013}.
\textit{Left column:} ordinary logistic regression (based on same
implementation but without bottleneck layer or autoencoder),
\textit{middle column:} semi-supervised low-rank logistic regression
(latent components $n=20$, $\lambda=0.5$, $\ell_1=0.1$, $\ell_2=0.1$),
\textit{right column:} voxel-wise average of whole-brain activity
across the first-level activity maps from 498 participants
(thresholded at 0.25).
All values z-scored.
}
\end{figure}

\begin{figure}
\begin{centering}
\includegraphics[width=1.00\textwidth]{figures/support_bars.png}
\end{centering}
\caption{\textbf{Relation between support recovery and out-of-dataset performance}
\textit{Upper row:} the fit-for-purpose of the classification models
is quantified by linear correlation between the
z-scored average task image and the z-scored weights from
ordinary (\textit{red}) and present (\textit{blue colors}) logistic
regression ($\lambda=0.25, 0.50, 0.75, 1.00$).
\textit{Lower row:} ordinary logistic regression is applied to the
ARCHI dataset after projection into the latent dimensions learned from
the HCP task data.
}
\end{figure}

\section{Experimental Results}
\paragraph{General}
\begin{itemize}
\item{20 components: high bias/low variance
100 compoentens: low bias/high variance}
\end{itemize}

\paragraph{Classification performance}
\begin{itemize}
  \item{This was evaluated by the prediction
accuracy on a validation set (20\% of the training data) at each iteration
over the ARCHI task data}
  \item{the less lambda, the faster the convergence, with small differences
  though; smaller differences with larger number of components}
  \item{both 20 and 100 hidden components achieve 97\% out-of-sample
  accuracy}
  \item{ordinary logistic regression converges faster but with stagnation at
  92\% out-of-sample accuracy}
  \item{classification at chance 6\% in case of lambda==0 because 
  only the unsupervised layer of the estimator is optimized}
  \item{important: the thus only resting-state-trained decomposition achieves
  the same usefulness in the ARCHI classification than the
  decomposition derived in semi-supervised fashion}
\end{itemize}

\paragraph{Support recovery}
\begin{itemize}
  \item{fit for purpose; the right model should have
  high probability in the right places}
  \item{comparing to ordinary logistic regression, low-rank logreg yielded class weights 
  that were much more similar to feature of the respective training examples,
  for any choice of $\lambda$}
  \item{the higher $\lambda$, the better support recovery}
  \item{conversely, the higher lambda (i.e., bigger emphasis on the
  resting-state autoencoder), the more useful the low-dimensional transformations
  for classification in an independent dataset (ARCHI) as measured by f1 score
  for higher (i.e., 50 and 100) but not lower (i.e., 20) numbers
  of latent components}
\end{itemize}

\paragraph{Anecdotal observations}
\begin{itemize}
  \item{Modifications of the modek that did not improve the generalization performance:
  drop-out, input corruption (p=0.1/0.3/0.5), RELUs}
  \item{no penalty or only L1 or L2 penalty generally yielded worse
  generalization performance}
  \item{low L1/L2 penalties (0.1) yielded better results
  than higher values (0.3, 0.5)}
  \item{``pretraining'' of the bottleneck by settings weights to
  preliminary PCA, SPCA or ICA weights did not give better accuracies,
  neither for autoencoder pretraining }
  \item{present approach better than first PCA, SPCA or ICA and
  subsequently Logreg on loadings, even if each on the whole data}
  \item{introducing a
  nonlinearity (sigmoid, tanh, softplus) into the system
  did not improve predictive accuracy but
  elastic did -> most useful decomposition has characteristics of a SPCA
  and not PCA or ICA}
  \item{introduced an addition layer with many units (50/100) did not
  improve generalization}
\end{itemize}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.40\textwidth]{figures/figure3.png}
  \end{center}
  \caption {\textbf{
  Equilibrium between autoencoder and low-rank logistic regression}
  One learned decomposition component (out of 20) between the only-autoencoder
  (\textit{uppermost panel}) and only-logistic-regression
  (\textit{lowermost panel}) scenarios.
  The congruent structure (\textit{red, middle column})
  in the
  posterior cingulate cortex, posterior mid-cingulate cortex, and medial
  prefrontal cortex appears to be important in decompositions of the rest and
  task neuroimaging data. With increasing weight on the supervised learning,
  non-congruent structure emerges in the early (\textit{blue}) and
  lateral (\textit{red}) visual cortex
  (\textit{right column}). Matrix weights were z-scored.
  }
\end{figure}


\section{Discussion and Conclusion}
%
There is an increasing occurrence of high-dimensional problems in the
neuroimaging domain. Yet, there is also considerable uncertainty about the
most pertinent representation
of neural activation information.
This calls for new statistical learning algorithms that
behave well in large samples. Ideally, they should acknowledge
and exploit widely-accepted neuroscientific knowledge.
We here propose such an estimator that learns
dimensionality reduction
in a neurobiological and interpretable fashion.

Using the flexibility of autoencoders, we
learn the optimal decomposition from high-dimensional
voxel brain space into the most important activation patterns for a
statistical learning question of interest.
The higher generalization accuracy and support recovery, comparing to
state-of-the-art classification, hold potential
for adoption in various every-day analyses in the neuroimaging field.
Besides increased performance, these models are more interpretable by
automatically learning a mapping to and from a brain-network space.
This domain-specifc classification algorithms
encourages departure from the artificial and statistically
less attractive voxel space.
From a neurobiological perspective,
we demonstrate that neural activity underlying defined mental operations
can readily be explained by linear combinations of the main activation
patterns. That is,
neural activity data as measured by fMRI probably concentrate near
a low-dimensional manifold of brain networks.
These fundamental building blocks of brain organization might
facilitate the quest for the cognitive primitives of
human thought.
Moreover, data-intense neuroimaging studies in the future can be made tractable
by reduction to the neurobiological essence.
This can be particulary useful in multi-center clinical studies
on the prediction of disease trajectories and drug treatment responses
in psychiatric and neurological populations.
We hope that these first steps stimulate development towards
powerful semi-supervised classification methods.

\paragraph{Acknowledgment}
{\small
Data were provided the Human Connectome Project. The study was supported
by the German National Academic Foundation (D.B.)}

\small
\bibliographystyle{splncs03}
\bibliography{nips_refs}

\end{document}