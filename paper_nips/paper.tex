\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{bbm}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{bbm}
\usepackage[titletoc]{appendix}
\usepackage{wrapfig}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{ulem}

\def\B#1{\bm{#1}}
%\def\B#1{\mathbf{#1}}
\def\trans{\mathsf{T}}

%\renewcommand{\labelitemi}{--}

\newtheorem{theorem}{Theorem} \newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Semi-supervised low-rank logistic regression for
high-dimensional neuroimaging data}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\prox}{prox}
\DeclareMathOperator{\Prox}{Prox}
\DeclareMathOperator{\im}{im}

% macros from michael's .tex
\DeclareMathOperator{\dist}{dist} % The distance.
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\abs}{abs}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}


\newcommand{\suggestadd}[1]{{\color{blue} #1}}
\newcommand{\suggestremove}[1]{{\color{red} \sout{#1}}}

\nipsfinalcopy % Uncomment for camera-ready version
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\author{Danilo Bzdok, Michael Eickenberg, Olivier Grisel,
  Bertrand Thirion,
  Ga\"el Varoquaux \\\\\textbf{\textit{email:} }firstname.lastname@inria.fr}

\maketitle

\begin{abstract}
Imaging neuroscience links human behavior to aspects of brain
biology in ever-increasing datasets.
%
Existing neuroimaging methods typically perform either discovery of unknown
neural structure or testing of neural structure associated with mental tasks.
%
However, testing hypotheses on the neural correlates of mental tasks
depends on the representation of the observations.
%
% XXX Michael: Added explicit outline of the model, description can be 
% XXX improved
We therefore propose to optimize
representation and task classification in
\suggestremove{an identical} \suggestadd{a unified} statistical learning 
problem\suggestadd{, by introducing a multinomial logistic regression with
joint rank constraint, which we regularize by forcing representational
capacity in the form of an autoencoder}.
%
We show that this approach yields more accurate and more interpretable
neural models of psychological tasks in a reference dataset.
%

% XXX Michael: Suggesting removal of bioinformatics tag,
% XXX see http://en.wikipedia.org/wiki/Bioinformatics
\textbf{keywords}: dimensionality reduction; semi-supervised learning;
\suggestremove{bioinformatics;} fMRI; systems neuroscience

\end{abstract}

\section{Introduction}
%
Methods for neuroimaging research can be grouped by discovering
neurobiological structure or assessing the neural correlates associated
with mental tasks.
To discover, on the one hand, spatial distributions of neural activity
structure across time,
independent component analysis (ICA) \cite{beckmann2005} is often used.
It decomposes the BOLD (blood-oxygen level-dependent) signals into the
primary modes of variation.
The ensuing spatial activity patterns are believed to represent
brain networks of
functionally interacting regions \cite{smith2009}.
Similarly, sparse principal component analysis (SPCA) \cite{varoqu2011}
has been used to
separate BOLD signals into parsimonious network components.
Thus extracted brain networks are probably
manifestations of electrophysiological oscillation frequencies \cite{hipp15}.
Their fundamental organizational role is further
attested by continued covariation during sleep and anesthesia \cite{fox07}.
%
Network discovery by applying ICA or SPCA is typically performed on
task-unrelated (i.e., \textit{unlabeled}) ``resting-state'' data.
These capture brain dynamics
during ongoing random thought without controlled environmental stimulation.
% XXX Michael: Added variation/variance, because otherwise that is trivial:
In fact, a large proportion of the BOLD signal \suggestadd{variation} is 
known
not to correlate with a particular behavior, stimulus, or experimental task
\cite{fox07}. 

To test, on the other hand,
the neural correlates underlying mental tasks,
the general linear model (GLM) is the dominant approach \cite{friston94}.
The contribution of
individual brain voxels is estimated
according to a design matrix of experimental tasks.
Alternatively, psychophysiological interactions (PPI)
elucidate the influence of one brain region on another conditioned
by experimental tasks \cite{friston97}.
% XXX Michael more and more or ever more:
As a last example, \suggestremove{always} \suggestadd{more and} more 
neuroimaging studies model
experimental tasks by training classification algorithms on brain signals
\cite{poldrack09decoding}.
All these methods are applied to task-associated (i.e., \textit{labeled})
data that capture brain dynamics
during stimulus-guided behavior.
Two important conclusions can be drawn.
First, the mentioned supervised neuroimaging analyses typically yield
results in a voxel space.
This ignores the fact that the BOLD
signal exhibits spatially distributed \suggestremove{activity} patterns
\suggestadd{of coherent activity}.
%
Second, existing supervised neuroimaging analyses
\suggestremove{do not acknowledge} \suggestadd{cannot exploit} the abundance
of easily acquired resting-state data \cite{biswaldiscovery}.
These may allow better \suggestremove{sampling} \suggestadd{discovery of} 
the manifold of brain states
due to the high task-rest similarities of neural activity patterns,
as observed using ICA \cite{smith2009}.

% XXX Michael: This parenthesis structure should probably be unravelled into
% XXX a sentence.
Both these biological properties can be conjointly exploited in a mixed
 (i.e., using task and rest data)
low-rank (i.e., performing network decomposition)
approach.
%
The integration of brain-network discovery into
supervised classification formally yields a semi-supervised learning
% XXX Michael: No, it doesn't necessarily. You can do that with labeled data
framework. The most relevant
neurobiological structure should hence be identified
for the prediction problem at hand.
%
Autoencoders suggest themselves because they can emulate
variants of most unsupervised learning algorithms,
including PCA, SPCA, and ICA \cite{hinton06}.
\begin{wrapfigure}{r}{0.40\textwidth}
  \centering
  %\captionsetup{labelformat=nonumber}
    \includegraphics[width=0.40\textwidth]{figures/figure1.png}
  \vspace{-0.7cm}
  \caption {\textbf{Architecture.}
  Linear autoencoders find an optimal compression of 79,941 brain voxels
  into $n$ unknown activity patterns by improving reconstruction from them.
  The decomposition matrix equates with the bottleneck of
  a factored logistic regression.
  Supervised multi-class learning on task data ($X_{task}$)
  is thus guided by
  unsupervised decomposition of rest data ($X_{rest}$).
  }
\end{wrapfigure}
%
Autoencoders
are \suggestremove{one-layered} learning models 
that condense the input data to
\suggestremove{local and global} \suggestadd{(typically lower-dimensional)} 
representations
by \suggestremove{improving} \suggestadd{optimizing}
reconstruction from them.
%
% XXX Michael: In fact, they behave like a non-centered PCA, 
% XXX which amounts to just doing an SVD
% XXX which btw is NOT a convex optimization problem, although easy to 
% XXX calculate and with a global optimum.
% XXX Autoencoders are also not one-layered by default. Let's make sure that
% XXX we don't trip up by getting the technicalities wrong and getting 
% XXX reviewers annoyed at us because of it.
They behave like a \suggestadd{(truncated, non-centered)} PCA
in case of one linear hidden layer and a squared error loss
\cite{baldi1989neural}.
This architecture yields a \suggestadd{non-}convex optimization objective
with unique global minimum.
Autoencoders behave like a SPCA if shrinkage terms are added to the
model weights in the optimization objective.
Moreover, they have the characteristics of an ICA in case of tied weights
and adding a nonlinear convex transformation \cite{le2011ica}.
% XXX What convex transformation? Why is that relevant? Convexity is only
% XXX relevant for the loss
These authors further demonstrated that ICA, sparse autoencoders, and 
sparse coding are mathematically equivalent
under mild conditions.
%
Hence, autoencoders can flexibly project the neuroimaging data
onto the main axes of variation and thus % XXX Michael <- that's PCA
reverse-engineer properties of the underlying
neural processes \cite{olshausen96}.
% XXX Michael ^ I don't get that last part nor the citation.

In the present investigation,
a linear autoencoder will be \suggestremove{fed by} \suggestadd{fit to} 
(unlabeled) rest data and
integrated as \suggestadd{a rank-reducing} bottleneck
into a \suggestremove{low-rank} \suggestadd{multinomial} 
logistic regression \suggestremove{fed by} \suggestadd{fit to} 
(labeled) task data.
We can then solve the unsupervised data representation and the
supervised classification in an identical statistical learning
problem.
%
From the perspective of dictionary learning, the first layer represents
% XXX Michael: This is the crucial difference: The first layer is an analysis
% XXX operation: it extracts information in some way. It may be extracting
% XXX activations for basis functions, in which case it would be the pseudo-
% XXX inverse of those basis functions. But unless they are orthogonal, these
% XXX weights definitely don't represent basis functions themselves. It is 
% XXX the second layer of the autoencoder that represents basis functions. If
% XXX weights are tied, they are one and the same thing, with interesting
% XXX implications. But in general, only the second layer represents a basis.
\suggestadd{projectors to} the discovered set of basis functions
whose linear combinations are learned
by the second layer \cite{olshausen96}.
% XXX Michael: ^ This needs to be changed around: This first layer obtains
% XXX activations, the second layer weights are basis functions which are
% XXX multiplied to these activations
%
Neurobiologically, this allows 
delineating a low-dimensional \suggestremove{manifold}
\suggestadd{vector space} of brain network patterns and then 
distinguishing mental tasks
by their most discriminative linear combinations.
%
Theoretically, a reduction in model variance should be achieved by
resting-state autoencoders that
put probability mass on the most neurobiologically
% XXX Michael: ^ MEH, we are not really in a probabilistic setting ...
valid models in the hypothesis set.
%
Taken together,
the important modes of variation in brain dynamics and
the neural correlates subserving mental operations
have mostly been studied in isolation.
% XXX Michael: Taken together, the important modes of variation in brain dynamics and the neural correlates subserving mental operations have mostly been studied in isolation? Are they taken together, or isolated? :)
We provide a principled computational framework to link these previously
unconnected domains of systems neuroscience.

\section{Methods}
%
\paragraph{Data.}
As the currently biggest openly-accessible reference dataset,
we chose the Human Connectome Project (HCP) resources
\cite{barch2013}.
Neuroimaging task data with labels of ongoing cognitive processes
were drawn from 500 unrelated,
healthy HCP participants.
18 HCP tasks 
were selected that are known to elicit reliable neural activity
across participants.
The task paradigms include
1) working memory/cognitive control processing, 2)
incentive processing, 3) visual and somatosensory-motor processing,
4) language processing (semantic and phonological processing),
5) social cognition, 6) relational processing, and 7) emotional
processing. All data were acquired on the same Siemens Skyra 3T scanner.
Whole-brain EPI acquisitions were acquired with a
32 channel head coil (TR=720ms, TE=33.1ms, flip angle=52°, BW=2290Hz/Px,
in-plane FOV=$280\textrm{mm}\times180\textrm{mm}$, 72 slices, 2.0mm 
isotropic voxels).
The ``minimally preprocessed'' pipeline includes
gradient unwarping, motion correction, fieldmap-based EPI distortion
correction, brain-boundary-based registration of EPI to structural
T1-weighted scans, nonlinear (FNIRT) registration into MNI space,
and grand-mean intensity normalization. Activity maps were spatially
smoothed with a Gaussian kernel of 4mm (FWHM). A GLM was
implemented by FILM from the FSL suite with model regressors from convolution
with a “canonical” hemodynamic response function and from temporal derivatives.
HCP tasks were conceived to modulate activity
in a maximum of different brain regions and neural systems. Indeed, at
least 70\% of the participants showed consistent brain activity in
contrasts from the task battery, which certifies excellent
coverage \cite{barch2013}.
%
% No, this is not what is called 'coverage'
%
In sum, the HCP task data incorporated 8650 first-level activity maps
from 18 diverse paradigms administered to 498 participants (2 removed
due to incomplete data).
All maps were resampled to a common 60x72x60 space of
3mm isotropic voxels and gray-matter masked (at least 10\% tissue
probability).
The supservised analyses were based on labeled HCP task maps with
79,941 voxels of interest representing z-values in gray matter.

These labeled data were complemented by unlabeled activity maps
from HCP acquisitions of unconstrained resting-state activity.
These reflect brain activity in the absence of controlled thought.
In line with the goal of the present study, acquisition of these data was
specifically aimed at the study of task-rest correspondence.
From each participant, we included two
time-series for left and right phase encoding
with 1,200 maps of multiband, gradient-echo planar imaging acquired
during a period of 15min (TR=720 ms, TE=33.1 ms, flip angle=52°,
FOV=$280\textrm{mm}\times180\textrm{mm}$, and 2.0mm isotropic voxels). 
Besides run duration,
the task acquisitions were identical to the resting-state fMRI acquisitions
for maximal compatibility between task and rest data.
We here drew on ``minimally preprocessed'' rest data
from 200 randomly selected healthy, unrelated participants.
PCA was applied to each set of 1,200 rest maps for
denoising by keeping only the 20 main modes of
variation.
In sum, the HCP rest data concatenated
8000 unlabeled, noise-cleaned rest maps with
40 brain images from each of 200 randomly selected participants.

We were further interested in the utility of the optimal low-rank projection
in one task dataset for dimensionality reduction in another task dataset.
To this end, the HCP-derived network decompositions were used as preliminary
step in the classifcation problem of another large sample.
The ARCHI dataset \cite{pinel07} provides activity maps from
diverse experimental tasks, including auditory and visual perception, motor action,
reading, language comprehension and mental calculation.
81 right-handed healthy participants
(3 not included in present analyses due to incomplete data)
without psychiatric or
neurological history participated in four fMRI sessions acquired under
different experimental paradigms.
The functional maps were warped into
the MNI space and resampled to isotropic 3mm resolution.
Whole-brain EPI data were acquired with the same Siemens Trio with a 32
channel head coil (TR=2400ms, TE=30ms, flip angle=60°, in-plane
FOV=$192\textrm{mm}\times192\textrm{mm}$, 40 slices, 3.0mm isotropic voxels).
Standard preprocessing was performed with Nipype \cite{gorgo11}, including
slice timing, motion correction, alignment, and spatial normalization.
Activity maps were spatially smoothed by
a Gaussian kernel of 5mm (FWHM).
Analogous to HCP data, the second task dataset incorporated 1404
labeled, grey-matter masked, and z-scored activity maps
from 18 diverse tasks acquired in 78 participants.

The labeled and unlabeled data were fed into a linear statistical model
composed of autoencoder and low-rank logistic regression.

\paragraph{Linear autoencoder.}
The affine autoencoder takes the input 
$\mathbf{x}$ and projects it into coordinates of a
latent representation $\mathbf{z}$
by

\begin{eqnarray}
  \begin{split}
    \mathbf{z} &= \mathbf{W_0} \mathbf{x} + \mathbf{b_0} \\
    \mathbf{x'} &= \mathbf{W_1} \mathbf{z} + \mathbf{b_1}
  \end{split}
  \label{eq:autoenc}
\end{eqnarray}

where $\mathbf{x \in \mathbb{R}^{d}}$ denotes the vector of $d=79{,}941$
voxel values from each
rest map,
$\mathbf{z \in \mathbb{R}^{n}}$ is the $n$\suggestremove{ hidden dimensions 
(i.e.,
distributed neural activity patterns)}
\suggestadd{-dimensional hidden state}, and 
$\mathbf{x' \in \mathbb{R}^{d}}$
is the reconstruction vector of the original activity map
from the hidden variables. 
% XXX use bold letters for vectors and matrices
Further, $\mathbf{W_0}$ denotes the weight matrix that
transforms
from input space into the hidden space (encoder),
$\mathbf{W_1}$ is the weight matrix for back-projection
from the hidden variables to the
output space (decoder).
$\mathbf{b_0}$ and $\mathbf{b_1}$ are bias vectors.
The optimal model parameters $\mathbf{W_0, b_0, b_1}$ are found by
minimizing the expected squared reconstruction error

\begin{equation}
  \mathbb E\left[{\mathcal{L_{AE}}}(\mathbf{x})\right] = %
  \mathbb E\left[\| \mathbf{x} - f_{AE}(\mathbf{x}) \|^2\right],
\end{equation}
with \(f_{AE}(\mathbf x) = %
\mathbf{W_1}(\mathbf{W_0}\mathbf{x} + \mathbf{b_0}) + \mathbf{b_1}\)
the autoencoder.
% XXX Michael: I don't know to what extent the following makes sense or is
% XXX correct, so I am commenting it
%% This reconstruction error criterion equates with
%% maximizing a lower bound on the mutual information between
%% input and the learned representation.
\suggestadd{
Here we choose $\mathbf{W_0}$ and $\mathbf{W_1}$ to be tied, i.e.
$\mathbf{W_0} = \mathbf{W_1^T}$. In particular, this means that the weights
learned are forced to take a two-fold function: That of signal 
\textit{analysis} and that of signal \textit{synthesis}.
Whereas the first layer \textit{analyzes}
the data in order to obtain the cleanest latent representation, the second
layer represents building blocks from which to \textit{synthesize} the data
using the latent activations. Tying these functionalities makes the analysis
side interpretable and pulls all non-zero singular values towards 1. 
Nonlinearities were not applied to the
activations in the first layer.
}
\paragraph{Reduced-rank logistic regression.}
Lossy compression by a low-dimensional bottleneck
is also imposed by the first layer of the low-rank
multinomial logistic regression.
It gives the probability of an input $\mathbf{x}$ to belong
to a class $i \in \{1, \dots, l\}$

\begin{equation}
  \begin{split}
    P(Y=i|\mathbf{x; V_0,V_1,c_0, c_1}) &= \softmax_i(f_{RL}(\mathbf x))
  \end{split}
  \label{eq:lr}
\end{equation}
where \(f_{LR}(\mathbf x) = \mathbf{V_1 (V_0 x + c_0) + c_1}\) computes 
multinomial logits and \(\softmax_i(x) = \exp(x_i)/\sum_j\exp(x_j)\).
The matrix $\mathbf{V_0 \in \mathbb{R}^{dxn}}$
transforms the input $\mathbf{x \in \mathbb{R}^{d}}$
into $n$ latent components
and the matrix $\mathbf{V_1 \in \mathbb{R}^{nxl}}$
projects the latent components
onto hyperplanes that reflect $l$ label probabilities.
$\mathbf{c_0}$ and $\mathbf{c_1}$ are corresponding
bias vectors.
The loss function is given by

\begin{equation}
    \mathbb E\left[{\mathcal{L_{LR}}}(\mathbf{x, y})\right] \approx %
\frac{1}{N_{X_{task}}} \sum_{k=0}^{N_{X_{task}}} \log(P(Y=y^{(k)}|\mathbf{x^{(k)}; V_0, V_1, c_0, c_1})
\label{eq:lr_loss}
\end{equation}

\paragraph{Layer combination.}
Importantly, the optimization problem of the linear autoencoder
and the low-rank logistic regression
are linked on two levels. First, their transformation matrices mapping from
input to the latent space are tied
\begin{eqnarray}
  \mathbf{V_0} = \mathbf{W_0}.
\end{eqnarray}
We thus search for a compression of the 79,941 voxel values into $n$ latent
components that represent an optimal latent code for both
rest and task activity data.
Second, the objectives of the autoencoder and the low-rank
logistic regression are interpolated in the common loss function

% XXX Michael: In resolving conflict I opted for my change. Feel free to revert if that was intended.
\begin{equation}
{\mathcal{L}}(\theta, \lambda) = \lambda {\mathcal{L_{LR}}}
+ (1-\lambda)\frac{1}{N_{X_{rest}}} {\mathcal{L_{AE}}} + \Omega(\theta).
  \label{eq:loss_equ}
\end{equation}

In so doing, we search for the combined model parameters
$\theta=\{\mathbf{V_0,V_1,c_0, c_1, b_0, b_1}\}$
with respect to the (unsupervised) reconstruction error and the
(supervised) task classification.
${\mathcal{L_{AE}}}$ is devided by ${N_{X_{rest}}}$ to equilibrate both
loss terms to the same order of magnitude.
\(\Omega(\theta)\) represents a regularization, for which we choose the
ElasticNet type, i.e. a combination of $\ell_1$ and $\ell_2$ penalties
$\forall p \in \theta$.

% XXX Michael: To sum up the math in this paragraph: We need to decide what
% XXX the \mathcal L are supposed to mean - sample wise loss or expected loss
% XXX or some global deterministic loss (the latter would be bad, because we)
% XXX are not optimizing it. Right now it looks like a sample-wise loss.

\paragraph{Optimization.}
The common objective was \suggestremove{approximated} \suggestadd{optimized}
% XXX why 'approximated' ?
in the neuroimaging data
by \suggestremove{updating parameters by derivates}
\suggestadd{gradient descent in the parameters}
% XXX derivates ???
of the 
semi-supervised low-rank logistic regression.
The required gradients are easily obtained by using the chain rule to
backpropagate error derivatives through the linear network.
As solver, we chose \textit{rmsprop} \cite{rmsprop},
a mini-batch version of rprop.
This procedure dictates an adaptive learning rate
for each model parameter by
scaled gradients from a running average.
Gradient normalization by rmsprop
is known to effectively exploit curvature information.
We opted for a small batch size of $100$, given the high degree of
redundancy in $X_{rest}$ and $X_{task}$.
The matrix parameters were initalized by Gaussian random values multiplied
by a gain of $0.004$. The bias parameters were initalized to zero.
With a slight abuse of notation, let $\theta$ denote a component of $\theta$.
The normalization factor and the update rule for $\theta$
are given by
% XXX Michael: It is crucial, but nowhere mentioned, that the optimization is
% XXX done in an alternating manner. The whole structure relies on this, by
% XXX construction it is vulnerable to optimizing too much towards labels or
% XXX rest at a time. I also identify this as one of the major weak points
% XXX of the method, so it needs to be stated in a way that is not easily
% XXX attackable.
% XXX Maybe we could also talk less about RMSprop.

\begin{eqnarray}
  \begin{split}
    \mathbf{v^{(t+1)}} &= \rho \mathbf{v^{(t+1)}} + (1 - \rho)\left(\frac{\partial f}{\partial \theta}\right)^2
%v^{(t+1)} = \rho v^{(t+1)} + (1 - \rho)˜|\nabla f
\\
\theta^{(t+1)} &= \theta^{(t)} + \alpha \frac{\nabla f(\theta^{(t)})}{\sqrt{\mathbf{v^{(t+1)}} + \epsilon}},
  \end{split}
\end{eqnarray}
% XXX Michael: Why, in the formula, do we have two different notations for
% XXX partial derivatives? \partial and \nabla

% XXX Michael: Probably remove the explanations following and just state values
where $0 < \rho < 1$ constitutes the decay rate. $\rho$ was set to
$0.9$ to deemphasize the magnitude of the gradient.
Further, $\alpha$ is the learning rate and $\epsilon$ a global damping factor.
The hyper-parameter $\alpha$ was set to $0.00001$ by prior studies and $\epsilon$ was set to $10^{-6}$.
%
Note that we have also experimented with other solvers
(stochastic gradient descent, adadelta, and adagrad) but found that
rmsprop converged faster and with higher generalization performance.

\paragraph{Hints.}
In fact, the constraint by a rest-data autoencoder qualifies as a
\textit{hint}
rather than regularization in a strict sense \cite{abu1994hints}.
Its purpose is not to prevent overfitting but to introduce
% XXX Michael: I use f earlier up, is this f here necessary?
prior knowledge on known properties of the unknown target function $f$.
Rather than only relying on input-output pairs in the learning process,
we thus narrow our hypothesis set to the biologically most plausible solutions.
That is, we reduce the search space in a way that
is compatible with the expected representation of BOLD activity signals.

\paragraph{Implementation.}
The analyses were performed in Python.
We used \textit{nilearn} to handle
the high-dimensional neuroimaging data 
\cite{abrah14}
and
\textit{Theano} for automatic, numerically stable
differentiation of symbolic computation graphs
\cite{bastien2012theano, bergstra2010theano}.
All Python scripts that generated the results are
accessible online for reproducibility and reuse
\url{http://github.com/anonymous/anonymous}.
% url{http://github.com/banilo/nips2015}.

\section{Experimental Results}
\paragraph{Serial versus parallel structure discovery and classification.}
We first tested for an advantage of combined unsupervised decomposition
and supervised classification learning.
We benchmarked against
performing data reduction on the (unlabeled)
first half of the HCP data by ICA and SPCA ($n=5, 20, 50, 100$ components)
and learning classification models in the (labeled) second half
by ordinary logistic regression.
%
ICA performed iterative blind source separation
by a parallel FASTICA implementation (200 maximum iterations,
per-iteration tolerance of 0.0001,
initialized by random mixing matrix, whitening).
SPCA separated the BOLD signals into
network components with few regions by
a regression-type optimization problem constrained by
$\ell_1$-penalty
(no orthogonality assumptions, 1000 maximum iterations,
per-iteration tolerance of 1 * 10\textsuperscript{-8}, sparsity
alpha=1).
%
The second half of the data was projected onto the
latent components discovered in first data half.
The ensuing component loadings were submitted to ordinary
logistic regression
(one hidden layer, $\ell_1=0.1$, $\ell_2=0.1$, 500 maximum iterations).
%
This 2-step approaches were compared against semi-supervised low-rank
logistic regression
(SS-LR LogReg, two hidden layers, $\ell_1=0.1$, $\ell_2=0.1$,
500 maximum iterations, $\lambda=0.75$).
Importantly, all trained classification models were tested
on a large, unseen test set (20\% of data) in the presented analyses.
%
Across choices for $n$, SS-LR LogReg
achieved more than 96\% out-of-sample accuracy, whereas
supervised learning based on ICA and SPCA loadings
ranged from 37,53\% to 87,28\%
and
32,19\% to 83,97\%, respectively (Table \ref{table_one}).
%
These explorations attest to the advantage of directly searching for
classification-relevant structure in the fMRI data,
rather than solving supervised and unsupervised problems independently.
This effect was particularly pronounced when assuming few hidden dimensions.

\begin{table}[h]
 \centering
 \resizebox{0.5\textwidth}{!}{%
 \begin{tabular}{lccc}
 \hline
 \textit{n}               & ICA + LogReg & SPCA + LogReg & SS-LR LogReg \\ \hline
 \multicolumn{1}{l|}{5}   & 37,53 \%     & 32,19 \%      & 96,50 \%     \\
 \multicolumn{1}{l|}{20}  & 80,98 \%     & 78,15 \%      & 97,33 \%     \\
 \multicolumn{1}{l|}{50}  & 84,19 \%     & 83,97 \%      & 97,69 \%     \\
 \multicolumn{1}{l|}{100} & 87,28 \%     & 82,19 \%      & 97,80 \%     \\ \hline
 \end{tabular}
 }
 \vspace{-0.2cm}
 \caption{Serial versus parallel dimensionality reduction and classification}
 \label{table_one}
 \end{table}

\paragraph{Model performance.}
SS-LR LogReg was subsequently trained ($500$ epochs) across parameter choices
for the hidden components ($n=5, 20, 100$) and
the balance between autoencoder and logistic regression
($\lambda=0.25, 0.5, 0.75, 1$).
Assuming 5 latent directions of variation should yield models with
higher bias and smaller variance then SS-LR LogReg with 100 latent directions.
%
Given the 18-class problem of HCP, setting $\lambda$ to $0$
consistently yields generalization performance
at chance-level (5,6\%) because
only the unsupervised layer of the estimator is optimized.
%
At each epoch (i.e., iteration over the data),
the out-of-sample performance of the trained classifier
was assessed on 20\% of unseen HCP data.
Additionally, the ``out-of-dataset performance'' of the learned decomposition
was assessed by using it as dimensionality reduction of an
independent labaled dataset (i.e., ARCHI) and conducting ordinary
logistic regression on the ensuing component loadings.

\begin{table}[h]
  \centering
  \resizebox{1.00\textwidth}{!}{%
\begin{tabular}{l|cccc|cccc|cccc|cccc|}
  \hline
                            & \multicolumn{4}{c|}{\textit{n = 5}}                                                                                                            & \multicolumn{4}{c|}{\textit{n = 20}}                                                                                                                   & \multicolumn{4}{c}{\textit{n = 100}}                                                       \\ \hline
  \textit{}                 & \multicolumn{1}{l}{$\lambda=0.25$} & \multicolumn{1}{l}{$\lambda=0.5$} & \multicolumn{1}{l}{$\lambda=0.75$} & \multicolumn{1}{l|}{$\lambda=1$} & \multicolumn{1}{l}{$\lambda=0.25$}   & \multicolumn{1}{l}{$\lambda=0.5$}   & \multicolumn{1}{l}{$\lambda=0.75$}   & \multicolumn{1}{l|}{$\lambda=1$}   & $\lambda=0.25$   & $\lambda=0.5$    & $\lambda=0.75$   & \multicolumn{1}{l}{$\lambda=1$}    \\ \hline
  Out-of-sample\\accuracy   & \textit{88,90\%}                   & 95,13\%                            & \textbf{96,49\%}                  & 95,72\%                          & 97,44\%                              & \textbf{97,80\%}                    & 97,33\%                              & \textit{97,32\%}                   & 97,21\%          & \textit{97,03\%} & \textbf{97,80\%} & 97,38\%                            \\
  Precision (mean)          & \textit{86,98\%}                   & 94,86\%                            & \textbf{96,28\%}                  & 95,36\%                          & \textbf{97,38\%}                     & 97,06\%                             & \textit{96,98\%}                     & 97,02\%                            & 96,90\%          & \textit{96,47\%} & \textbf{97,50\%} & 96,91\%                            \\
  Recall (mean)             & \textit{88,27\%}                   & 95,19\%                            & \textbf{96,57\%}                  & 95,70\%                          & \textbf{97,51\%}                     & 97,50\%                             & \textit{97,35\%}                     & 97,36\%                            & 97,20\%          & \textit{97,15\%} & \textbf{97,88\%} & 97,37\%                            \\
  F1 score (mean)           & \textit{86,64\%}                   & 94,89\%                            & \textbf{96,38\%}                  & 95,43\%                          & \textbf{97,42\%}                     & 97,22\%                             & \textit{97,11\%}                     & 97,13\%                            & 97,00\%          & \textit{96,71\%} & \textbf{97,65\%} & 97,07\%                            \\ \hline
  Out-of-dataset \\accuracy & 60,83\%                            & \textit{54,30\%}                   & 60,69\%                           & \textbf{62,92\%}                 & 79,72\%                              & \textbf{81,94\%} & 79,72\%          & \textit{79,44\%}                     & \textbf{82,08\%} & 81,66\%         & 81,25\%          & \textit{75,83\%}                   \\ \hline
  \end{tabular}
}
\vspace{-0.2cm}
 \caption{Performance of SS-LR LogReg across model parameter choices}
  \label{table_two}
\end{table}

We made several noteworthy observations.
%
First, the purely supervised estimator ($\lambda=1$) achieved in no
instance the best accuracy, precision, recall or f1 scores on HCP data.
Classification by low-rank logistic regression is therefore facilitated by
imposing structure from the unlabeled rest data.
%
Second, the higher the number of latent components $n$,
the higher the out-of-dataset performance with small choices for $\lambda$.
This suggests that the presence of more rest-data-inspired hidden components
yields to more effective feature representation in unrelated task data.
%
Third, for $n=20$ and $100$ (but not $5$) the purely rest-data-trained
decomposition matrix ($\lambda=0$) resulted in
noninferior out-of-dataset performance
of X and Y, respectively (not shown in Table \ref{table_two}).
This confirms that guiding model learning by task-unrelated structure
extract features of general relevance
beyond the supervised problem at hand.

Depicts f1 scores for prediction of each of 38 classes across training epochs.
Ordinary logistic regression operating in voxel space (\textit{left plot})
converged faster but performed worse than
low-rank logistic regression for few
(\textit{middle plot}) and many (\textit{right plot}).
Autoencoder or rest data were not used for these analyses
($\lambda=1$).
Hence, projecting the input data into a reduced space for classification
yields higher class separability.

\paragraph{Individual effects of dimensionality reduction and rest data.}
We first quantified the gain of introducing a bottleneck layer
disregaring the autoencoder.
To this end, ordinary logstic regression was juxtaposed
with LR LogRed at $\lambda=1$ (no autoencoder).
For this experiment, we increased the difficulty of the classification problem
by including data from all 38 HCP tasks.
Indeed, increased class separability in component space,
as compared to voxel space, entails differences in f1 score of
$\approx{17\%}$ (Figure \ref{fig_dimred}).

\begin{figure}
\begin{centering}
\includegraphics[width=1.00\textwidth]{figures/accuracies.png}
\end{centering}
\vspace{-0.5cm}
\caption{\textbf{Effect of bottleneck in a 38-task classificaton problem}
Depicts f1 scores for prediction of each of 38 classes across training epochs.
Ordinary logistic regression operating in voxel space (\textit{left plot})
converged faster but performed worse than
low-rank logistic regression for few
(\textit{middle plot}) and many (\textit{right plot}).
Autoencoder or rest data were not used for these analyses
($\lambda=1$).
Hence, projecting the input data into a reduced space for classification
yields higher class separability.
}
\label{fig_dimred}
\end{figure}

We then quantified the impact of structure from rest data keeping
all model parameters constant
($n=20$, $\lambda=0.5$, $\ell_1=0.1$, $\ell_2=0.1$).
At the beginning of every epoch,
2000 task and 2000 rest maps were drawn with replacement
from same amounts of task maps but varying amounts of rest maps.
In frequently encountered data-scarce settings ($\approx{100}$ samples),
but not in data-rich settings ($\approx{1000}$ samples),
the repertoire of rest structure modulates model performance
(Figure \ref{fig_semisup}).

\begin{figure}
\begin{centering}
\includegraphics[width=1.00\textwidth]{figures/semisup_both.png}
\end{centering}
\vspace{-0.5cm}
\caption{\textbf{Effect of richness in rest data}
Gradient descent was performed on 2000 task and 2000 rest images.
These were drawn with replacement from identical versus varying
quantities of task versus rest maps, at the begining of each epoch.
}
\label{fig_semisup}
\end{figure}

\paragraph{Feature identification.}
We finally examined whether the models
were fit for purpose
(Figure \ref{fig_weights}).
%
To this end, we computed Pearson's correlation between the classifier weights
and the averaged neural activity map for each of the 18 tasks.
%
Ordinary logistic regression thus yielded a mean feature identification
of $\rho=0.28$.
%
For SS-LR LogReg ($\lambda=0.25, 0.5, 0.75, 1$),
a per-class-weight map was computed by matrix
multiplication of the two inner layers.
Feature identification thus ranged
between $\rho=0.35$ and $\rho=0.55$ for $n=5$,
between $\rho=0.59$ and $\rho=0.69$ for $n=20$, and
between $\rho=0.58$ and $\rho=0.69$ for $n=100$.
%
Consequently,
SS-LR LogReg puts higher absolute weights on relevant structure.
This reflect a higher signal-to-noise ratio, in part explained
by the more BOLD-typical local contiguity.
%
Conversely, SS-LR LogReg puts lower probability mass on irrelevant structure.
Despite lower interpretability, the
salt-and-pepper-like weight maps obtained from
ordinary logistic regression are sufficient for
good classification performance.
%
Hence, SS-LR LogReg yielded class weights 
that were much more similar to features of the respective training samples
for any choice of $\lambda$.
SS-LR LogReg captures genuine properties of neural activity patterns,
rather than participant- or paradigm-specific artefacts.

\begin{figure}
\begin{centering}
\includegraphics[width=0.98\textwidth]{figures/figure_weights_perc75.png}
\end{centering}
\vspace{-0.1cm}
\caption{\textbf{Classification weight maps}
The voxel predictors corresponding to 5 exemplary
(of 18 total) psychological tasks (\textit{rows})
from the HCP dataset \cite{barch2013}.
\textit{Left column:} ordinary logistic regression (same
implementation but without bottleneck or autoencoder),
\textit{middle column:} semi-supervised low-rank logistic regression
($n=20$ latent components, $\lambda=0.5$, $\ell_1=0.1$, $\ell_2=0.1$),
\textit{right column:} voxel-wise average of whole-brain
activity maps from each task.
Low-rank logistic regression
a) puts higher absolute weights on relevant structure
b) lower ones on irrelevant structure,
and
c) yields BOLD-typical local contiguity.
All values are z-scored and thresholded at the 75th percentile.
}
\label{fig_weights}
\end{figure}

\paragraph{Miscellaneous observations.}
For the sake of completeness,
we informally report modifications of the model
that did not improve the generalization performance.
%
Introducing stochasticity into model learning by 
a) input corruption of $\mathbf{X_{task}}$ or
b) drop-out at the bottleneck $\mathbf{W_0}$ using binomial
distributions ($p=0.1, 0.3, 0.5$) deteriorated model performance
in all instances. 
%
Adding c) rectified linear units (ReLU) to $\mathbf{W_0}$ or
other commonly used nonlinearities (d) sigmoid, e) softplus,
f) hyperbolic tangent) all led to smaller classification accuracies.
%
Further, g) ``pretraining'' of the bottleneck $\mathbf{W_0}$
(i.e., non-random initialization) by 
either corresponding SPCA or ICA loadings did not exhibit improved accuracies,
neither did h) autoencoder pretraining.
%
Moreover,
introducing an additional i) overcomplete layer (100 units)
after the bottleneck was not advantageous.
%
Finally, imposing imposing either j) only $\ell_1$-penalty or 
k) only $\ell_2$-penalty was disadvantageous in all tested cases,
which favored elasticnet regularization chosen in the above analyses.

\section{Discussion and Conclusion}
% Conceptual recap+gain of current paper
Using the flexibility of autoencoders, we
learn the optimal decomposition from high-dimensional
voxel brain space into the most important activity patterns for a
supervised learning question.
The higher generalization accuracy and support recovery, comparing to
ordinary logistic regression, hold potential
for adoption in various neuroimaging analyses.
Besides increased performance, these models are more interpretable by
automatically learning a mapping to and from a brain-network space.
This domain-specifc classification algorithms
encourages departure from the artificial and statistically
less attractive voxel space.
Neurobiologically,
neural activity underlying defined mental operations
can be explained by linear combinations of the main activity
patterns. That is,
fMRI data probably concentrate near
a low-dimensional manifold of brain networks.
Extracting fundamental building blocks of brain organization might
facilitate the quest for the cognitive primitives of
human thought.
We hope that these first steps stimulate development towards
powerful semi-supervised classification methods in systems neuroscience.

% grand perspective
In the future, automatic reduction of brain images to
their neurobiological essence
may leverage data-intense neuroimaging studies.
Initiatives for data collection are rapidly increasing
in neuroscience \cite{poldrack2014data}.
These promise structured integration
of accumulating neuroscientific knowledge from neuroimaging databases.
Tractability by
condensed feature representations can avoid the ill-posed problem of
learning the full distribution of activity patterns.
%
This is not only relevant to the 
multi-class challenges spanning the human cognitive space
\cite{schwartz2013mapping}
but also the
multi-modal combination with
high-resolution 3D models of brain anatomy \cite{amunts2013bigbrain}
and
high-throughput genome analyses \cite{need2010gwas}.
%
The biggest socioeconomic potential may
lie in across-hospital clinical studies that
predict disease trajectories and drug responses
in psychiatric/neurological populations
\cite{frackowiak2015future, gustav2011cost}.

\paragraph{Acknowledgment}
{\small
Data were provided by the Human Connectome Project. The study was supported
by the German National Academic Foundation (D.B.).
}

\small
\bibliographystyle{splncs03}
\bibliography{nips_refs}

\end{document}
